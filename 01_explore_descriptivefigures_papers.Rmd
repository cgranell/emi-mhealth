---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

## Metadata

Required libraries and runtime environment description.

```{r load_libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(emidata)
library(tidyverse)
library(tidytext)
library(kableExtra)
library(stringr)
```


```{r session_info, echo=FALSE}
devtools::session_info(include_base = TRUE)
```

This document is an EDA notebook to explore descriptive and informative figures of the dataset *papers* in the *emidata* package. Type `?papers` for information. 

## Data 

The data for the analysis is in the *papers* dataset that has `` `r nrow(papers)` `` references.

```{r data_source, echo=TRUE}
papers %>% str()
```

As part of the text analysis of papers, including wordcloud and terms frequency analysis, read the full list of abstracts from the `papers` and process them to create a [tidy](https://www.jstatsoft.org/article/view/v059i10) data structure without [stop words](https://en.wikipedia.org/wiki/Stop_words).

```{r stopwords, echo=FALSE}
tidy_abstracts <- papers %>%
  select(id, filename, abstract) %>%
  arrange(id)
  
  
papers_words <- tidy_abstracts %>%
    select(id, abstract) %>%
    unnest_tokens(word, abstract)

my_stop_words <- tibble(
  word = c(
    "et",
    "al",
    "fig",
    "e.g",
    "i.e",
    "http",
    "ing",
    "pp",
    "figure",
    "based"
    ),
  lexicon = "jmir")


all_stop_words <- stop_words %>%
  bind_rows(my_stop_words)

# Get rid of numeric values (as words) from abstracts
suppressWarnings({
  no_numbers <- papers_words %>%
    filter(is.na(as.numeric(word)))
})

# Get list of words from abstracts without stopwords 
no_stop_words <- no_numbers %>%
  anti_join(all_stop_words, by = "word") 

```


```{r calculate_stopword_stats, echo=TRUE}
total_words = nrow(papers_words)
after_cleanup = nrow(no_stop_words)
```

About `r round(after_cleanup/total_words * 100)` % of the words are considered stop words.

_How many non-stop words does each abstract have?_

```{r stop_words}

non_stop_words_per_paper <- no_stop_words %>%
  group_by(id) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

ggplot(non_stop_words_per_paper, aes(id, num_words)) + geom_col() 

kable(no_stop_words %>%
  group_by(id) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)))
```


## Search terms in the abstract

### Psychological terms

The detection matches full words using regex option `\b`.

- mental (and also e-mental)
- psycholog (`psycholog.*`, i.e. psychology, psychological)
- psychiatric
- emotional
- health (and also e-health, u-health)
- treatment(s) (and also pretreatment, post-treatment)
- disorder(s) 
- intervention(s) 
- therapy(ies)
- distress,
- affection
- depressi (`depressi.*`, i.e. depression, depressive)
- anxiety,
- ecological momentary intervention

```{r psy_search_terms_per_abstract, echo=FALSE}
tidy_abstracts_lower <- stringr::str_to_lower(tidy_abstracts$abstract)
psy_terms_count <- tibble(
  id = tidy_abstracts$id,
  mental = stringr::str_count(tidy_abstracts_lower, "\\bmental\\b"),
  `psycholog..` = str_count(tidy_abstracts_lower, "\\bpsycholog.*\\b"),
  psychiatric = stringr::str_count(tidy_abstracts_lower, "\\bpsychiatric\\b"),
  emotional = stringr::str_count(tidy_abstracts_lower, "\\bemotional\\b"),
  health = stringr::str_count(tidy_abstracts_lower, "\\bhealth\\b"),
  `treatment(s)` = stringr::str_count(tidy_abstracts_lower, "\\btreatments?\\b|\\bpretreatment?\\b"),
  `disorder(s)` = stringr::str_count(tidy_abstracts_lower, "\\bdisorder?\\b"),
  `intervention(s)` = stringr::str_count(tidy_abstracts_lower, "\\bintervention?\\b"),
  `therapy/ies` = stringr::str_count(tidy_abstracts_lower, "\\btherap(y|ies)\\b"),
  distress = stringr::str_count(tidy_abstracts_lower, "\\bdistress\\b"),
  affection = stringr::str_count(tidy_abstracts_lower, "\\baffection\\b"),
  `depressi..` = str_count(tidy_abstracts_lower, "\\bdepressi.*\\b"),
  anxiety = stringr::str_count(tidy_abstracts_lower, "\\banxiety\\b"))
  # emi = stringr::str_count(tidy_abstracts_lower, "\\becological momentary intervention\\b)"))
  # TODO; I cannot search by several words separated by blanck spaces (EMI case)

# sum a bunch of columns row-wise
# https://stackoverflow.com/a/32827260/261210
sumColsInARow <- function(df, list_of_cols, new_col) {
  df %>% 
    mutate_(.dots = ~Reduce(`+`, .[list_of_cols])) %>% 
    setNames(c(names(df), new_col))
}

# Sum all occurences of psy search terms row-wise (i.e. per abstract) and put result in a new column "all"
psy_terms_count_sum <- sumColsInARow(
  psy_terms_count, names(psy_terms_count)[names(psy_terms_count) != "id"], "all") %>%
  arrange(desc(all))

# Compute totals per column, i.e per each psy search term and "all"
psy_terms_count_sum_total <- psy_terms_count_sum %>% 
  summarise_if(is.numeric, funs(sum)) %>%
  add_column(id = "Total", .before = 0)
  
psy_terms_count_sum <- rbind(psy_terms_count_sum, psy_terms_count_sum_total)
```

_How often do psychological related search terms appear in each abstract?_

```{r  psy_search_terms_table, echo=TRUE}
# testing
# kable(psy_terms_count_sum)
kable(psy_terms_count_sum,
      caption = paste0("Psychological-related search terms in the corpus,",
                       " ordered by sum of matches per abstract"),
      format = "html", booktabs = TRUE) %>%
  kableExtra::landscape()
```



### Technical terms

*Search terms*: cell, mobile smartphone, smart, portable, phone*, device*, app, apps, applicat*, mhealth, uhealth, ehealth, emental, android, iphone


_How often do Computer science related search terms appear in each abstract?_


## Frequency analysis onf top words in the abstracts

_Word cloud of abstracts_ 
_Word cloud of top words_

