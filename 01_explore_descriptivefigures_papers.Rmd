---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

## Metadata

Required libraries and runtime environment description.

```{r load_libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(emidata)
library(tidyverse)
library(tidytext)
library(kableExtra)
library(stringr)
library(wordcloud)
library(skimr)

```


```{r session_info, echo=FALSE}
devtools::session_info(include_base = TRUE)
```

This document is an EDA notebook to explore descriptive and informative figures of the dataset *papers* in the *emidata* package. Type `?papers` for information. 

## Data 

The data for the analysis is in the *papers* dataset that has `` `r nrow(papers)` `` references.

```{r data_source, echo=FALSE}
papers %>% glimpse()
papers %>% skimr::skim()
```

As part of the text analysis of papers, including wordcloud and terms frequency analysis, read the full list of abstracts from the `papers` and process them to create a [tidy](https://www.jstatsoft.org/article/view/v059i10) data structure without [stop words](https://en.wikipedia.org/wiki/Stop_words).

```{r stopwords, echo=FALSE}
tidy_abstracts <- papers %>%
  select(id, filename, abstract) %>%
  arrange(id)
  
  
papers_words <- tidy_abstracts %>%
    select(id, filename, abstract) %>%
    unnest_tokens(word, abstract)


my_stop_words <- tibble(
  word = c(
    "et",
    "al",
    "fig",
    "e.g",
    "i.e",
    "http",
    "ing",
    "pp",
    "figure",
    "based",
    "Ã¢",
    "background", # used to structure an abstract
    "objective",
    "methods",
    "results",
    "conclusions"
    ),
  lexicon = "jmir")


all_stop_words <- stop_words %>%
  bind_rows(my_stop_words)

# Get rid of numeric values (as words) from abstracts
suppressWarnings({
  no_numbers <- papers_words %>%
    filter(is.na(as.numeric(word)))
})

# Get list of words from abstracts without stopwords 
no_stop_words <- no_numbers %>%
  anti_join(all_stop_words, by = "word") 

```


```{r calculate_stopword_stats, echo=FALSE}
total_words = nrow(papers_words)
after_cleanup = nrow(no_stop_words)
```

About `r round(after_cleanup/total_words * 100)` % (`r after_cleanup`) of the words in all abstracts are considered stop words.

_How many non-stop words does each abstract have?_

```{r stop_words, echo=FALSE}

non_stop_words_per_abstract <- no_stop_words %>%
  group_by(filename) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

#ggplot(non_stop_words_per_abstract, aes(id, num_words)) + geom_col() 

kable(non_stop_words_per_abstract,
  caption = "Abstracts ordered by number of (significant) words after deleting stopwords.", 
  format = "html", booktabs = TRUE)
```


## Search terms in the abstract

### Psychological terms

The detection matches full words using regex option `\b`.

- mental (and also e-mental)
- psycholog (`psycholog.*`, i.e. psychology, psychological)
- psychiatric
- emotional
- health (and also e-health, u-health)
- treatment(s) (and also pretreatment, post-treatment)
- disorder(s) 
- intervention(s) 
- therapy(ies)
- distress,
- affection
- depressi (`depressi.*`, i.e. depression, depressive)
- anxiety,
- ecological momentary intervention

```{r psy_search_terms_per_abstract, echo=FALSE}
tidy_abstracts_lower <- stringr::str_to_lower(tidy_abstracts$abstract)
psy_terms_count <- tibble(
  id = tidy_abstracts$id,
  mental = stringr::str_count(tidy_abstracts_lower, "\\bmental\\b"),
  `psycholog..` = str_count(tidy_abstracts_lower, "\\bpsycholog.*\\b"),
  psychiatric = stringr::str_count(tidy_abstracts_lower, "\\bpsychiatric\\b"),
  emotional = stringr::str_count(tidy_abstracts_lower, "\\bemotional\\b"),
  health = stringr::str_count(tidy_abstracts_lower, "\\bhealth\\b"),
  `treatment(s)` = stringr::str_count(tidy_abstracts_lower, "\\btreatments?\\b|\\bpretreatment?\\b"),
  `disorder(s)` = stringr::str_count(tidy_abstracts_lower, "\\bdisorder?\\b"),
  `intervention(s)` = stringr::str_count(tidy_abstracts_lower, "\\bintervention?\\b"),
  `therapy/ies` = stringr::str_count(tidy_abstracts_lower, "\\btherap(y|ies)\\b"),
  distress = stringr::str_count(tidy_abstracts_lower, "\\bdistress\\b"),
  affection = stringr::str_count(tidy_abstracts_lower, "\\baffection\\b"),
  `depressi..` = str_count(tidy_abstracts_lower, "\\bdepressi.*\\b"),
  anxiety = stringr::str_count(tidy_abstracts_lower, "\\banxiety\\b"))
  # emi = stringr::str_count(tidy_abstracts_lower, "\\becological momentary intervention\\b)"))
  # TODO; I cannot search by several words separated by blanck spaces (EMI case)

# sum a bunch of columns row-wise
# https://stackoverflow.com/a/32827260/261210
sumColsInARow <- function(df, list_of_cols, new_col) {
  df %>% 
    mutate_(.dots = ~Reduce(`+`, .[list_of_cols])) %>% 
    setNames(c(names(df), new_col))
}

# Sum all occurences of psy search terms row-wise (i.e. per abstract) and put result in a new column "all"
psy_terms_count_sum <- sumColsInARow(
  psy_terms_count, names(psy_terms_count)[names(psy_terms_count) != "id"], "all") %>%
  arrange(desc(all))

# Compute totals per column, i.e per each psy search term and "all"
psy_terms_count_sum_total <- psy_terms_count_sum %>% 
  summarise_if(is.numeric, funs(sum)) %>%
  add_column(id = "Total", .before = 0)
  
psy_terms_count_sum <- rbind(psy_terms_count_sum, psy_terms_count_sum_total)
```

_How often do psychological related search terms appear in each abstract?_

```{r  psy_search_terms_table, echo=FALSE}
# for testing
# kable(psy_terms_count_sum)
kable(psy_terms_count_sum,
      caption = paste0("Psychological-related search terms in the corpus,",
                       " ordered by sum of matches per abstract"),
      format = "html", booktabs = TRUE) %>%
  kableExtra::landscape()
```

### Technical terms

The detection matches full words using regex option `\b`.

- cell (and cell-phone, cellphone)
- mobile
- smart (and smartphone)
- portable
- phone(s) 
- device(s)
- app(s)
- applicat (`applicat.*`, i.e. application)
- emotional
- mhealth
- uhealth
- ehealth
- emntal
- android
- iphone

_How often do Computer science related search terms appear in each abstract?_


## Frequency analysis of top words (most frequent) in the abstracts


### Figure: Word cloud of asbtracts (A), psyschology top words, and technical top words


```{r top_words, echo=FALSE}

countPapersUsingWord <- function(the_word) {
  sapply(the_word, function(w) {
    no_stop_words %>%
      filter(word == w) %>%
      group_by(id) %>%
      count %>%
      nrow
  })
}

# top25 words
no_top_words <- 25
top_words <- no_stop_words %>%
  group_by(word) %>%
  tally %>%
  arrange(desc(n)) %>%
  head(no_top_words) %>%
  mutate(`# abstracts` = countPapersUsingWord(word)) %>%
  add_column(place = c(1:nrow(.)), .before = 0)

```

_Word cloud of `r no_top_words` top words in abstracts_ 

```{r Fig_Word_cloud,dpi=600,fig.width=8,fig.asp=0.85, echo=FALSE}
set.seed(1)
if (max(top_words$n) < 100) {
  minimum_occurence <- round(mean(top_words$n))
} else {
  minimum_occurence <-  min(top_words$n) # Or arbritary number like 100 
}

cloud_words <- no_stop_words %>%
  group_by(word) %>%
  tally %>%
  filter(n >= minimum_occurence) %>% 
  arrange(desc(n))

if (nrow(cloud_words) > 0) {  
  plot.new()
  wordcloud(cloud_words$word, cloud_words$n,
            max.words = Inf,
            random.order = FALSE,
            fixed.asp = FALSE,
            rot.per = 0,
            #color = brewer.pal(8,"Dark2"))
            color = brewer.pal(9,"BuGn"))
} else {
  warning("No input data for wordcloud.")
}



```


This word cloud is based on `r length(unique(cloud_words$word))` unique words occuring each at least `r minimum_occurence` times, all in all occuring `r sum(cloud_words$n)` times which is roughly `r round(sum(cloud_words$n)/ nrow(no_stop_words) * 100)` % of the all non-stop words.


```{r  top_words_table, echo=FALSE}
# for testing
# kable(top_words)
kable(top_words,
      caption = paste0("Occurrences of top",no_top_words," words in the corpus"),
      format = "html", booktabs = TRUE)

```

```{r next_steps}
#TODO: How do top words and search terms correlate?
# Do it manually or automated. Which of the top words are also search terms? Are search terms in the top of the list? Are there standing top words that were nt search terms?
#TODO: Visualise table of occurrances of top words
#TODO: How do the top words correlated with the top keywords in the corpus?
#TODO: How do the top words correlated with the top keywords in each abstract?

```
 

